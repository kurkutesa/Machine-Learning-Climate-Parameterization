{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "dzqucxn8IKHt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Virtual environment\n",
        "#! export PATH=\"/scratch/$USER/miniconda/base/bin:$PATH\"\n",
        "#! source activate venv\n",
        "\n",
        "import time\n",
        "# MATLAB like tic toc\n",
        "def TicTocGenerator():\n",
        "    ti = 0\n",
        "    tf = time.time()\n",
        "    while True:\n",
        "        ti = tf\n",
        "        tf = time.time()\n",
        "        yield tf-ti\n",
        "TicToc = TicTocGenerator() # create an instance of the TicTocGen generator\n",
        "def toc(tempBool=True):\n",
        "    tempTimeInterval = next(TicToc)\n",
        "    if tempBool:\n",
        "        print( \"Elapsed time: {:.1f} seconds.\".format(tempTimeInterval) )\n",
        "def tic():\n",
        "    toc(False)\n",
        "tic()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "35YWCId0iHZ1",
        "colab_type": "code",
        "outputId": "360b69b2-b308-4301-a305-c66edc9d8d7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Import pkg\n",
        "import matplotlib\n",
        "matplotlib.use('TkAgg')\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from math import floor, ceil\n",
        "#import xarray as xr\n",
        "import datetime as dt\n",
        "import smtplib\n",
        "import tensorflow as tf\n",
        "print('All packages imported.')\n",
        "toc()\n",
        "\n",
        "# Reproducibility\n",
        "random_state = 42\n",
        "np.random.seed(random_state)\n",
        "tf.set_random_seed(random_state)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All packages imported.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "A4nn-VGObc0_",
        "colab_type": "code",
        "outputId": "5d74d99e-f358-41cb-a058-8017a2c76b14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "cell_type": "code",
      "source": [
        "# Mount Google Drive locally\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# Check data list\n",
        "!ls \"/content/gdrive/My Drive/Colab Notebooks/data/\"\n",
        "!ls '/tmp'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "ARM_1hrlater.csv  ARM_no_dropna.cdf  ARM_strict_dropna.csv\n",
            "drivefs_ipc.0  drivefs_ipc.0_shell\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "L1-zWXeTifl4",
        "colab_type": "code",
        "outputId": "17db78c4-2513-44af-865d-52354a89a2d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Read data\n",
        "DATADIR = '/content/gdrive/My Drive/Colab Notebooks/data'\n",
        "f = DATADIR + '/ARM_1hrlater.csv'\n",
        "df = pd.read_csv(f,index_col=0) # the first column in .csv is index\n",
        "\n",
        "# Double check NaN does not exist\n",
        "print('There are {} NaN in the data.'.format(df.isnull().sum().sum()))\n",
        "#df"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 0 NaN in the data.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lkV2jrnkoSIt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Generate inputs and labels\n",
        "input = df.drop(columns='prec_sfc_1hrlater')\n",
        "label = df['prec_sfc_1hrlater']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Gdg94ppburnm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#################### OBSOLETE - data standardization with numpy \n",
        "# Split data\n",
        "train_size = 0.75\n",
        "train_cnt = floor(input.shape[0] * train_size)\n",
        "\n",
        "x_train = input.iloc[0:train_cnt].copy().values\n",
        "y_train = label.iloc[0:train_cnt].copy().values.reshape([-1,1])\n",
        "x_test = input.iloc[train_cnt:].copy().values\n",
        "y_test = label.iloc[train_cnt:].copy().values.reshape([-1,1])\n",
        "\n",
        "# Normalize everything using mean/std of training data\n",
        "norm_mean, norm_std = [], []\n",
        "for col in range(x_train.shape[1]):\n",
        "  _mean = x_train[:,col].mean()\n",
        "  _std = x_train[:,col].std()\n",
        "  x_train[:,col] = (x_train[:,col] - _mean)/ _std\n",
        "  x_test[:,col] = (x_test[:,col] - _mean)/ _std\n",
        "  \n",
        "  norm_mean = np.append(norm_mean, _mean)\n",
        "  norm_std = np.append(norm_std, _std)\n",
        "\n",
        "# All precipitation uses the same normalization constants\n",
        "prec_mean, prec_std = norm_mean[2], norm_std[2]\n",
        "y_train = (y_train - prec_mean)/ prec_std\n",
        "y_test = (y_test - prec_mean)/ prec_std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OfI1gTSlwa-a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Split data\n",
        "train_size = 0.75\n",
        "train_cnt = floor(input.shape[0] * train_size)\n",
        "\n",
        "x_train = input.iloc[0:train_cnt].copy().values\n",
        "y_train = label.iloc[0:train_cnt].copy().values.reshape([-1,1])\n",
        "x_test = input.iloc[train_cnt:].copy().values\n",
        "y_test = label.iloc[train_cnt:].copy().values.reshape([-1,1])\n",
        "\n",
        "# Normalize data\n",
        "INPUT_PRE_NORM = tf.placeholder(\"float\", [None, None], name='pre_norm')\n",
        "mean, variance = tf.nn.moments(INPUT_PRE_NORM, [0], name='moments') # batch normalization\n",
        "std = tf.sqrt(variance)\n",
        "\n",
        "NORM_MEAN = tf.placeholder(\"float\", [None])\n",
        "NORM_STD = tf.placeholder(\"float\", [None])\n",
        "normalized = (INPUT_PRE_NORM - NORM_MEAN) / NORM_STD\n",
        "with tf.Session() as sess:  \n",
        "  # Normalize everything using mean/std of training data\n",
        "  _mean, _std = sess.run([mean, std], feed_dict = {INPUT_PRE_NORM: x_train})\n",
        "  x_train = sess.run(normalized, feed_dict = {INPUT_PRE_NORM: x_train, \n",
        "                                              NORM_MEAN: _mean, \n",
        "                                              NORM_STD: _std})\n",
        "  # Double check _mean_0, _std_1 are all zeros and ones\n",
        "  #_mean_0, _std_1 = sess.run([mean, std], feed_dict = {INPUT_PRE_NORM: x_train})\n",
        "  #print(_mean_0, _std_1)\n",
        "  x_test = sess.run(normalized, feed_dict = {INPUT_PRE_NORM: x_test, \n",
        "                                             NORM_MEAN: _mean, \n",
        "                                             NORM_STD: _std})\n",
        "  \n",
        "  # All precipitation uses the same normalization constants\n",
        "  y_train = sess.run(normalized, feed_dict = {INPUT_PRE_NORM: y_train, \n",
        "                                              NORM_MEAN: np.atleast_1d(_mean[2]), \n",
        "                                              NORM_STD: np.atleast_1d(_std[2])})\n",
        "  y_test = sess.run(normalized, feed_dict = {INPUT_PRE_NORM: y_test, \n",
        "                                             NORM_MEAN: np.atleast_1d(_mean[2]), \n",
        "                                             NORM_STD: np.atleast_1d(_std[2])})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ya8u2Gy2lXpm",
        "colab_type": "code",
        "outputId": "f37e2614-2f58-46c8-e453-1710db0e1d17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "cell_type": "code",
      "source": [
        "# Run Tensorboard in the background\n",
        "LOGDIR = '/tmp/log'\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOGDIR)\n",
        ")\n",
        "\n",
        "# Use ngrok to tunnel traffic to localhost\n",
        "! wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "! unzip ngrok-stable-linux-amd64.zip\n",
        "get_ipython().system_raw('./ngrok http 6006 &')\n",
        "\n",
        "# Retrieve public url\n",
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-12-11 15:14:14--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 34.232.181.106, 34.235.97.255, 34.231.75.48, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|34.232.181.106|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5363700 (5.1M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip.1’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]   5.11M  9.68MB/s    in 0.5s    \n",
            "\n",
            "2018-12-11 15:14:14 (9.68 MB/s) - ‘ngrok-stable-linux-amd64.zip.1’ saved [5363700/5363700]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "replace ngrok? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: ngrok                   \n",
            "https://d5b842b9.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "g6_s9B0XvwGl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Network Parameters\n",
        "n_in = x_train.shape[1] # number of input\n",
        "#n_hid_1 = 256 #neuron in the 1st layer\n",
        "#n_hid_2 = 128\n",
        "#n_hid_3 = 64\n",
        "#n_hid_4 = 32\n",
        "n_out = y_train.shape[1] # number of output\n",
        "n_hid = [n_in, 256, 128, 64, 32, n_out]\n",
        "\n",
        "# Create layer template\n",
        "def layer(x, size_in, size_out, relu_bool=True, name='layer'):\n",
        "  with tf.name_scope(name):\n",
        "    with tf.name_scope('weights'):\n",
        "      weight = tf.Variable(tf.random_normal([size_in, size_out], stddev=1), name='weight')\n",
        "      tf.summary.histogram('weights', weight)\n",
        "    with tf.name_scope('biases'):\n",
        "      bias = tf.Variable(tf.constant(0.1, shape=[size_out]), name='bias')\n",
        "      tf.summary.histogram('biases', bias)\n",
        "      \n",
        "    with tf.name_scope('pre_activations'):\n",
        "      _layer = tf.add(tf.matmul(x,weight), bias)\n",
        "      tf.summary.histogram('pre_activations', _layer)\n",
        "    \n",
        "    if relu_bool:\n",
        "      with tf.name_scope('activations'):\n",
        "        _layer = tf.nn.relu(_layer)\n",
        "        tf.summary.histogram('activations', _layer)\n",
        "      \n",
        "    return _layer\n",
        "  \n",
        "# Batch normalization template\n",
        "def bn(x, trainable, name='bn'):\n",
        "  with tf.name_scope('batch_norm'):\n",
        "    _mean, _var, _epsilon, _offset, _scale = 0, 1, 1e-4, 0, 1\n",
        "    _layer = tf.nn.batch_normalization(x, mean=_mean, variance=_var, offset=_offset, scale=_scale, variance_epsilon=_epsilon, name='batch_norm')\n",
        "      \n",
        "  return _layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hffjYG6JwnlP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Training parameters\n",
        "num_epoch = 100000\n",
        "batch_size = 727 # 727*5 batch = 3635 training sample\n",
        "display_epoch = 5000\n",
        "summ_epoch = 5\n",
        "\n",
        "# Create NN model \n",
        "def neural_net(learning_rate, hparam):\n",
        "  MODELDIR = '/tmp/' + hparam + '/model.ckpt'\n",
        "  tf.reset_default_graph() # clear graph stack\n",
        "  sess = tf.Session() # declare a session\n",
        "  \n",
        "  # tf Graph input\n",
        "  X = tf.placeholder(\"float\", [None, n_in], name='inputs')\n",
        "  Y = tf.placeholder(\"float\", [None, n_out], name='labels')\n",
        "  \n",
        "  # Layer connection\n",
        "  #layer_0 = bn(X, False, 'bn_0')\n",
        "  layer_1 = layer(X, n_in, n_hid[1], True,  'layer_1')\n",
        "  layer_2 = layer(layer_1, n_hid[1], n_hid[2], True, 'layer_2')\n",
        "  layer_3 = layer(layer_2, n_hid[2], n_hid[3], True, 'layer_3')\n",
        "  layer_4 = layer(layer_3, n_hid[3], n_hid[4], True, 'layer_4')\n",
        "  layer_out = layer(layer_4, n_hid[4], n_out, False, 'layer_out')\n",
        "  \n",
        "  # True data information\n",
        "  with tf.name_scope('constant'):\n",
        "    _prec_mean = tf.constant(prec_mean.astype(np.float32), name='prec_mean')\n",
        "    _prec_std = tf.constant(prec_std.astype(np.float32), name='prec_std')\n",
        "  \n",
        "  # Loss function\n",
        "  with tf.name_scope('losses'):\n",
        "    loss = tf.reduce_mean(tf.square(tf.square(layer_out - Y)), name='loss') # mean-quartic-error\n",
        "    trueloss = tf.reduce_mean(tf.multiply(tf.abs(layer_out - Y), _prec_std) + _prec_mean) # De-normalized mean loss\n",
        "  tf.summary.scalar('loss', loss)\n",
        "  tf.summary.scalar('trueloss', trueloss)\n",
        "  \n",
        "  # Optimizer\n",
        "  with tf.name_scope('train'):\n",
        "    #optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
        "  \n",
        "  summ = tf.summary.merge_all() # merge all summaries for Tensorboard\n",
        "  saver = tf.train.Saver() # declare NN config saver\n",
        "  \n",
        "  # Draw graph\n",
        "  sess.run(tf.global_variables_initializer()) # initialize session\n",
        "  \n",
        "  writer = tf.summary.FileWriter(LOGDIR + '/' + hparam) # a file writer\n",
        "  writer.add_graph(sess.graph) # write the graph in the session\n",
        "  \n",
        "  # Train\n",
        "  for epoch in range(1, num_epoch+1):\n",
        "    # Batching\n",
        "    loss_avg = 0.0\n",
        "    total_batch = int(len(x_train) / batch_size)\n",
        "    x_batches = np.array_split(x_train, total_batch)\n",
        "    y_batches = np.array_split(y_train, total_batch)     \n",
        "    for i in range(total_batch):\n",
        "      batch_x, batch_y = x_batches[i], y_batches[i]\n",
        "      _opt, _loss, _summ = sess.run([optimizer, trueloss, summ], feed_dict={X: batch_x,\n",
        "                                                                            Y: batch_y})           \n",
        "      loss_avg += _loss / total_batch\n",
        "    \n",
        "    if epoch % summ_epoch == 0:\n",
        "      writer.add_summary(_summ, epoch)\n",
        "    \n",
        "    if epoch % display_epoch == 0:\n",
        "      print(\"epoch \" + str(epoch) + \", training mean true loss=\" + \"{:.5f}\".format(np.sqrt(loss_avg)))\n",
        "  print(\"Optimization Finished!\")\n",
        "  \n",
        "  # Save NN model\n",
        "  save_path = saver.save(sess, MODELDIR)\n",
        "  print(\"Model saved in path: %s\" % save_path)\n",
        "\n",
        "  # Test\n",
        "  loss_test, output_test = sess.run([trueloss, layer_out], feed_dict={X: x_test,\n",
        "                                                                      Y: y_test})\n",
        "  print(\"testing mean true loss=\" + \"{:.5f}\".format(np.sqrt(loss_test)))\n",
        "  sess.close()\n",
        "  return loss_test, output_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uY0crbMNwrAI",
        "colab_type": "code",
        "outputId": "b2c41333-f401-4bdc-dd59-e00fac478e3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1068
        }
      },
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-3\n",
        "\n",
        "# Log hyperparameter with folder name\n",
        "def make_hparam_str(learning_rate, n_hid):\n",
        "  string = 'lr_{:.0e},nn'.format(learning_rate)\n",
        "  for n in n_hid:\n",
        "    string = string + '_{}'.format(n)\n",
        "  return string\n",
        "\n",
        "# Construct model\n",
        "hparam = make_hparam_str(learning_rate, n_hid)\n",
        "print(hparam)\n",
        "loss_test, output_test = neural_net(learning_rate, hparam)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lr_1e-03,nn_151_256_128_64_32_1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-e6d8fb9e06c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mhparam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_hparam_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_hid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mloss_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneural_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-16-9d57aef9f028>\u001b[0m in \u001b[0;36mneural_net\u001b[0;34m(learning_rate, hparam)\u001b[0m\n\u001b[1;32m     58\u001b[0m       \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_batches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m       _opt, _loss, _summ = sess.run([optimizer, trueloss, summ], feed_dict={X: batch_x,\n\u001b[0;32m---> 60\u001b[0;31m                                                                             Y: batch_y})           \n\u001b[0m\u001b[1;32m     61\u001b[0m       \u001b[0mloss_avg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0m_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtotal_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "gSIoRSNKuWY9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Save log and model\n",
        "!mv /tmp /content/gdrive/My\\ Drive/Colab\\ Notebooks/tmp\n",
        "\n",
        "# (Alternative) Download model\n",
        "#from google.colab import files\n",
        "#!apt-get install zip && zip -r tmp.zip /tmp\n",
        "#files.download('tmp.zip')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XpOmo8JVw68M",
        "colab_type": "code",
        "outputId": "3d370b5a-5cb2-4794-8857-ea7b4d7cb81f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "np.abs((output_test-y_test)* prec_std + prec_mean).max()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1072.6562033728535"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "ebVwj3cDkGue",
        "colab_type": "code",
        "outputId": "930a96b8-7364-41b3-d9cd-6b7510d4b139",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "cell_type": "code",
      "source": [
        "# Plot\n",
        "outputs = output_test * prec_std + prec_mean\n",
        "labels = y_test * prec_std + prec_mean\n",
        "\n",
        "plt.scatter(labels, outputs)\n",
        "\n",
        "plt.xlabel('True precipitation')\n",
        "plt.ylabel('Predicted precipitation')\n",
        "axes = plt.gca()\n",
        "#axes.set_xlim([0,20])\n",
        "#axes.set_ylim([0,20])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-57eae5f8cf15>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_test\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mprec_std\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mprec_mean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mprec_std\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mprec_mean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'output_test' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "W4tv-Q79Rqlh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}